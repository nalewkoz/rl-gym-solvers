{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hbtw9KpG7zCi"
   },
   "source": [
    "### Q-learning agent solving the FrozenLake8x8-v0 environment from the Gym package. \n",
    "\n",
    "Main classes: `discreteQlearningAgent` and `gymTrainer`. \n",
    "\n",
    "To do:\n",
    "- Inherit all agents from a common (abstract) class. Check if a loaded agent is correct before the training (or test).\n",
    "- Add a function that checks sizes of state and action spaces in a given (discrete) environments and test if the same agent implementations can be used in other environments.\n",
    "- Implement other agents:\n",
    "    - SARSA.\n",
    "    - Policy gradient.\n",
    "    - Model based (e.g., Dyna-Q).\n",
    "    - A better handcrafted agent (graph search + heuristics).  \n",
    "- Add and test a potential-based shaping reward function [Ng, A. Y., Harada, D., & Russell, S. (1999, June). Policy invariance under reward transformations: Theory and application to reward shaping. In Icml (Vol. 99, pp. 278-287).](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/NgHaradaRussell-shaping-ICML1999.pdf)\n",
    "- Non-tabular (functional approximator) training for randomized maps.\n",
    "- Functional approximators for non-discrete environments.\n",
    "\n",
    "Comments\n",
    "- Looking at the Q function I can see some interesting (rather simple) tactics. For example, it's often better not to go directly \"where we want\", but instead to go in the direction opposite to H. \n",
    "- Two related reasons why it is useful to switch between greedy and stochastic policies:\n",
    "    - We test how the greedy (with respect to the estimated Q-function) performs. That way we can gauge the real progress.\n",
    "    - Stochastic (softmax) policy rarely gets to the goal, because it is not easy to tune $\\beta$ (this is the reason why I randomize $\\beta$). This slows down the traning. Greedy policies, on the other hand, may not explore enough. This is why it may be a good idea to interleave both policies during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "72ELleKJsiJ2"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import rlsolvers    \n",
    "\n",
    "path_models = \"models/\"\n",
    "\n",
    "# Create an instance of the FrozeLake8x8-v0 or FrozenLake8x8-v1 environment\n",
    "env = gym.make('FrozenLake8x8-v1')#, map_name=None)\n",
    "# Instantiate an agent (discreteQlearningAgent)\n",
    "agent = rlsolvers.discreteQlearningAgent()\n",
    "# Instantiate a gymTrainer\n",
    "trainer = rlsolvers.gymTrainer(env, agent)\n",
    "# It is also possible to specify the path with a pickle containing the agent:\n",
    "# trainer = gymTrainer(env, \"agentQ.pickle\")\n",
    "\n",
    "# Train the agent for Nepisodes steps. By default this will also visualize the training.\n",
    "# If save_name is specified as a string (filename), \n",
    "# the agent will be automatically saved to the file (pickle) at the end of the training. \n",
    "trainer.train(Nepisodes=20000, save_name=path_models+\"agentQL.pickle\")\n",
    "\n",
    "# Test the trained agent in the trainer.\n",
    "print(\"== Trained agent (tabular Q-learning): ==\")\n",
    "trainer.test()\n",
    "\n",
    "# Test another agent (loaded externally) using the same trainer. Note that this will not update the agent for future training.\n",
    "print(\"== Simple handcrafted agent: ==\")\n",
    "agentHC = rlsolvers.testAgent()   # Handcrafted agent\n",
    "trainer.test(agentHC)\n",
    "\n",
    "# Test an agent loaded directly from a specified path.\n",
    "print(\"== agentQL.pickle: ==\")\n",
    "trainer.test(path_models+\"agentQL.pickle\")\n",
    "\n",
    "# Save the trained agent whenever needed. \n",
    "# Note that agentXXX.pickle and agentQL.pickle should be identical\n",
    "trainer.save_agent(path_models+\"agentXXX.pickle\");\n",
    "\n",
    "# Load an agent into the trainer.\n",
    "trainer.load_agent(agentHC)\n",
    "\n",
    "# Save the agent.\n",
    "trainer.save_agent(path_models+\"agentHC.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "yGgty11Id5bK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.19.2\n",
      "3.3.2\n",
      "0.21.0\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)\n",
    "import matplotlib as mpl\n",
    "print(mpl.__version__)\n",
    "print(gym.__version__)\n",
    "import pickle\n",
    "print(pickle.format_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.5 (default, Sep  4 2020, 07:30:14) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNhec4U9FjqHAYgVrkj10nT",
   "collapsed_sections": [],
   "name": "gat-ml-frozenlake.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
