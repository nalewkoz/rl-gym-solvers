{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gat-ml-frozenlake.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNhec4U9FjqHAYgVrkj10nT"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hbtw9KpG7zCi"
      },
      "source": [
        "Q-learning agent solving the FrozenLake8x8-v0 environment from the Gym package. \n",
        "Main classes: `discreteQlearningAgent` and `gymTrainer`. \n",
        "\n",
        "To do:\n",
        "- Inherit all agents from a common (abstract) class. Check if a loaded agent is correct before the training (or test).\n",
        "- Add a function that checks sizes of state and action spaces in a given (discrete) environments and test if the same agent implementations can be used in other environments.\n",
        "- Implement other agents:\n",
        "    - SARSA.\n",
        "    - Policy gradient.\n",
        "    - Model based (e.g., Dyna-Q).\n",
        "    - A better handcrafted agent (graph search + heuristics).  \n",
        "- Add and test a potential-based shaping reward function [Ng, A. Y., Harada, D., & Russell, S. (1999, June). Policy invariance under reward transformations: Theory and application to reward shaping. In Icml (Vol. 99, pp. 278-287).](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/NgHaradaRussell-shaping-ICML1999.pdf)\n",
        "- Non-tabular (functional approximator) training for randomized maps.\n",
        "- Functional approximators for non-discrete environments.\n",
        "\n",
        "Comments\n",
        "- Looking at the Q function I can see some interesting (rather simple) tactics. For example, it's often better not to go directly \"where we want\", but instead to go in the direction opposite to H. \n",
        "- Two related reasons why it is useful to switch between greedy and stochastic policies:\n",
        "    - We test how the greedy (with respect to the estimated Q-function) performs. That way we can gauge the real progress.\n",
        "    - Stochastic (softmax) policy rarely gets to the goal, because it is not easy to tune $\\beta$ (this is the reason why I randomize $\\beta$). This slows down the traning. Greedy policies, on the other hand, may not explore enough. This is why it may be a good idea to interleave both policies during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72ELleKJsiJ2"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "# By default the FrozenLake environment has an internal maximum of 200 steps (done is set to True after 200 steps).\n",
        "# Because of that, we do not really need the constant below. However, it is possible to remove this restriction in the environment. \n",
        "# In that case we can restrict the number of steps taken in the environment at the level of this module using the constant below.\n",
        "MAX_LEN_EPISODE = 201    \n",
        "\n",
        "# In FrozenLake environment actions are encoded as follows:\n",
        "# 0: LEFT\n",
        "# 1: DOWN\n",
        "# 2: RIGHT\n",
        "# 3: UP\n",
        "\n",
        "# ------------RL agent (tabular Q-learning)-----------------\n",
        "class discreteQlearningAgent:\n",
        "    \"\"\"A class representing an agent solving the task defined in the OpenAI Gym FrozenLake8x8-v0 environment. \n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    ... TO DO !!!!!!\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    new_episode(state=0):\n",
        "        Resets attributes representing state (s) and action (a). Should be \n",
        "        called at the beginning of each episode.\n",
        "    set_alpha(alpha=0, step = 0, scheduler=True):\n",
        "        Sets a new value of the learning rate (alpha).\n",
        "    set_beta(beta=0, step = 0, scheduler=True):\n",
        "        Sets a new value of the inverse temperature (beta).\n",
        "    select_action(greedy=False):\n",
        "        Selects action.\n",
        "    update(self, obs, r, done, learn = True):\n",
        "        Update the state and Q-function (observation + learning).\n",
        "\n",
        "    Comments\n",
        "    --------\n",
        "    Currently, only tabular Q-learning is implemented. This is fine if the map is fixed, but \n",
        "    will not generalize to new maps. A more powerful representation with a functional \n",
        "    approximation (ConvNet?) is needed to solve a more general (and much more interesting!) \n",
        "    problem of an environment in which the map is randomly generated in each episode.\n",
        "\n",
        "    This implementation is not very general. In particular, the number of states and actions \n",
        "    is not inferred from the environment and as such this class by default only deals with \n",
        "    the specific environment it was designed for (FrozenLake8x8-v0). \n",
        "    However, the user can set Nstates and Nactions during the creation of an object, \n",
        "    which should allow this class to handle any Discrete action and state Space from the Gym (not tested).\n",
        "\n",
        "    It is also possible to write a more generic code using Spaces: env.observation_space and env.action_space,\n",
        "    see: https://gym.openai.com/docs/#spaces.\n",
        "\n",
        "    A more elegant way would be to introduce an abstract class of an agent (\"baseAgent\") (e.g., using abc).\n",
        "    That would allow us to check if an object 'agent' is an instance of the class baseAgent. \n",
        "    Due to limited time I won't take this approach here.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, gamma=0.99, alpha0=0.1, alpha_tau = 10000, beta0=3, beta_tau = 1e9, random_beta = True, state=0, Qinit=1, Nstates = 64, Nactions = 4):\n",
        "        \"\"\"Initialize the agent.\"\"\"\n",
        "        # Initialize tabular Q function.\n",
        "        # Shape should be (# of states, # of actions) = (64, 4). \n",
        "        # Actions (from 0 to 3): left, down, right, up.\n",
        "        self.Q  = np.zeros( (Nstates, Nactions) ) + Qinit # Start with an optimistic agent (to encourage exploration). \n",
        "\n",
        "        # Initialize learning parameters.\n",
        "        self.gamma     = gamma\n",
        "        self.alpha     = alpha0\n",
        "        self.alpha0    = alpha0\n",
        "        self.alpha_tau = alpha_tau\n",
        "        \n",
        "        # Initialize action selection parameters.\n",
        "        self.beta0       = beta0\n",
        "        self.beta_tau    = beta_tau\n",
        "        self.random_beta = random_beta\n",
        "\n",
        "        # Set variables that should be reset at the beginning of each episode.\n",
        "        self.new_episode(episode = 0) \n",
        "\n",
        "    def new_episode(self, episode=None, state=0):\n",
        "        # Initialize the state.\n",
        "        self.s  = state\n",
        "        # Initialize the action variable (None, because no action was taken so far)\n",
        "        self.a  = None\n",
        "        # Either add 1 to the episode number, or set it externally\n",
        "        if episode is None:\n",
        "            self.episode += 1\n",
        "            episode = self.episode\n",
        "        else:\n",
        "            self.episode = episode\n",
        "        # Set a new value of alpha.\n",
        "        self.set_alpha(step = episode)\n",
        "        # Set a new value of beta.\n",
        "        self.set_beta(step = episode, random=self.random_beta)\n",
        "\n",
        "    def select_action(self, greedy=False):\n",
        "        \"\"\"Select and return an action.\n",
        "\n",
        "        Available policies:\n",
        "        softmax and greedy. \n",
        "\n",
        "        Parameters:\n",
        "        s -- current state\n",
        "        Q -- tabular Q function (Q[state,action])\n",
        "        beta -- inverse temperature of the softmax function\n",
        "        greedy (default False) -- if True, a deterministic (greedy with respect to Q) selection is used (effectively beta=infinity) \n",
        "        \n",
        "        Returns: an integer (action)\n",
        "        \"\"\" \n",
        "        if greedy:\n",
        "            self.a = np.argmax(self.Q[self.s,:]) \n",
        "            return self.a\n",
        "        \n",
        "        probs = np.exp(self.beta*self.Q[self.s,:])\n",
        "        probs = probs/np.sum(probs)\n",
        "\n",
        "        self.a = np.random.choice(len(probs), p=probs) \n",
        "        return self.a\n",
        "    \n",
        "    def update(self, obs, r, done, learn = True):\n",
        "        \"\"\"Update the state of the agent as well as the Q-function estimator (if learn==True).\n",
        "        \n",
        "        The agent must have taken an action before.\n",
        "        \"\"\"\n",
        "        if learn:\n",
        "            if self.a is None:\n",
        "                raise Exception(\"Cannot update because the agent did not take any action so far. Call select_action() first.\")\n",
        "            # Update the Q function using the TD error.\n",
        "            old_prediction = self.Q[self.s, self.a] \n",
        "            if done:\n",
        "                # There will not be any next step here, so we should not \n",
        "                # bootstrap (add our prediction of future rewards).\n",
        "                new_prediction = r \n",
        "            else:\n",
        "                new_prediction = r + self.gamma*np.max(self.Q[obs,:])\n",
        "\n",
        "            # Update the action-value function\n",
        "            self.Q[self.s, self.a] += self.alpha*(new_prediction - old_prediction)\n",
        "\n",
        "        # Update the state.\n",
        "        self.s = obs\n",
        "\n",
        "    def set_alpha(self, alpha=0, step = 0, scheduler=True):\n",
        "        \"\"\"Set the learning rate. \n",
        "\n",
        "        If scheduler is False alpha is set manually.\n",
        "        Otherwise the argument step should be provided which \n",
        "        allows this function to determine the value of alpha according \n",
        "        to the scheduler.\n",
        "        \"\"\" \n",
        "        if not scheduler:\n",
        "            self.alpha = alpha\n",
        "        else:\n",
        "            # Currently only ~1/t scheduler implemented. \n",
        "            self.alpha = self.alpha0/(1 + step/self.alpha_tau)\n",
        "        return self.alpha\n",
        "\n",
        "    def set_beta(self, beta=0, step = 0, scheduler=True, random=False):\n",
        "        \"\"\"Set an inverse temperature for the softmax action selection. \n",
        "\n",
        "        If scheduler=False beta is set manually.\n",
        "        Otherwise parameter step should be provided which allows \n",
        "        this function to determine the value of beta according \n",
        "        to the scheduler.\n",
        "        Two schedulers are available:\n",
        "        (1) non-random: beta increases linearly in time.\n",
        "        (2) random: beta is generated randomly from an exponential distribution.\n",
        "        \"\"\" \n",
        "        if not scheduler:\n",
        "            self.beta = beta\n",
        "        else:\n",
        "            # Currently only ~1/t scheduler implemented.\n",
        "            if random:\n",
        "                self.beta = -self.beta0*np.log(np.random.rand())\n",
        "            else: \n",
        "                self.beta = self.beta0 + step/self.beta_tau\n",
        "        return self.beta\n",
        "\n",
        "# ------------Handcrafted agent-----------------\n",
        "class testAgent(discreteQlearningAgent):\n",
        "    \"\"\"A simple handcrafted agent solving the task defined in the OpenAI Gym FrozenLake8x8-v0 environment. \n",
        "\n",
        "    Strategy: \n",
        "        (1) go up if in the top row. This will lead to one of three outcomes: stay, left, or right.\n",
        "        (2) go right if the the first column from the right. This will lead to one of three outcomes: stay, up, or down.\n",
        "    Given this strategy, the agent should never move outside of of top-right strips. Thus, the agent will be \"surprised\" \n",
        "    to find itself in such a situation and will throw an exception.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def select_action(self, greedy=False):\n",
        "        if self.s < 7:      # Top row excluding the top right corner\n",
        "            self.a = 3      # go UP\n",
        "        elif self.s%8 == 7: # The first column from the right \n",
        "            self.a = 2      # go RIGHT\n",
        "        else:\n",
        "            raise Exception(f\"I did not expect this state: {self.s}\")\n",
        "        \n",
        "        return self.a    \n",
        "\n",
        "# ------------Trainer (gym wrapper) class--------------------\n",
        "class gymTrainer:\n",
        "    \"\"\"A trainer class for OpenAI Gym. \n",
        "\n",
        "    Tested only in a single environment (FrozenLake8x8-v0), \n",
        "    but it should also work fine in other environments. \n",
        "    \n",
        "    Methods:\n",
        "    -------\n",
        "    train: trains an agent. The agent can be specified during class instantation. \n",
        "    It can also  be provided in the train method; in this case the environment will store \n",
        "    that agent for future trainings and tests.\n",
        "    \n",
        "    test: tests an agent. A greedy policy of the agent will be used (agent's select_action method must provide a flag greedy).\n",
        "    \n",
        "    save_agent: save an agent to a file.\n",
        "\n",
        "    load_agent: load an agent from a file or set the 'student' agent to an agent loaded externally.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, env, agent=None):\n",
        "        \"\"\"Initialize an instance of the gymTrainer class.\n",
        "\n",
        "        env   -- gym environment, must be loaded externally.\n",
        "        agent -- either an instance of discreteQlearningAgent class, \n",
        "        or a filename of a corresponding pickle. \n",
        "        \"\"\"\n",
        "        self.env   = env\n",
        "        self.load_agent(agent)\n",
        "        self.reset_logs()\n",
        "\n",
        "\n",
        "    def train(self, agent=None, Nepisodes=1000, visualize=True, save_name=None):\n",
        "        \"\"\"Train an agent.\n",
        "\n",
        "        agent can be:\n",
        "        None -- use the agent that is stored in self.agent\n",
        "        str  -- load the agent from a file \n",
        "        discreteQlearningAgent -- or, more generally, an instance of a class that provides methods new_episode, select_action, and update.\n",
        "\n",
        "        Note that this method will replace self.agent with a new agent, if provided. \n",
        "        In this case it will also reset training logs.\n",
        "        \"\"\"\n",
        "        if agent is not None:\n",
        "            # Load a new agent.\n",
        "            self.load_agent(agent)\n",
        "            # Reset logs.\n",
        "            self.reset_logs()\n",
        "        else:\n",
        "            if self.agent is None:\n",
        "                raise Exception(\"No agent to train (self.agent is None)\")\n",
        "\n",
        "        agent = self.agent\n",
        "\n",
        "        for i in range(Nepisodes):\n",
        "            env.reset()\n",
        "            try:\n",
        "                agent.new_episode()\n",
        "            except AttributeError:\n",
        "                raise Exception(\"agent: method new_episode not implemented correctly.\")\n",
        "            # Alternate between greedy and stochastic policy. \n",
        "            # This should allow the agent to explore more, but also to \n",
        "            # make sure that it can obtain a positive reward more often. \n",
        "            # Additionally, this allows us to better visualize the learning progress. \n",
        "            # The observed performance of the greedy policy is a proxy to \n",
        "            # how well the Q-function has been learned so far.\n",
        "            if i%2:\n",
        "                greedy = True\n",
        "            else:\n",
        "                greedy = False\n",
        "            for j in range(MAX_LEN_EPISODE):\n",
        "                # Select an action.\n",
        "                try:\n",
        "                    action = agent.select_action(greedy=greedy)\n",
        "                except AttributeError:\n",
        "                    raise Exception(\"agent: method select_action not implemented correctly.\")\n",
        "                # Take a step in the environment using our selected action and collect observations.\n",
        "                obs, r, done, info = env.step(action)\n",
        "                # Update agent's state and Q-function.\n",
        "                try:\n",
        "                    agent.update(obs, r, done)\n",
        "                except AttributeError:\n",
        "                    raise Exception(\"agent: method update not implemented correctly.\")\n",
        "                # If the episode is done, update the history.\n",
        "                if done:\n",
        "                    if greedy:\n",
        "                        self.log_greedy.append( (i, r, j+1) )\n",
        "                    else:             \n",
        "                        self.log_noisy.append( (i, r, j+1) )\n",
        "                    break\n",
        "                if j+1==MAX_LEN_EPISODE:\n",
        "                    raise Exception(f\"The length of the episode has hit the maximum number: {MAX_LEN_EPISODE}\")\n",
        "        if visualize:\n",
        "            self.__visualize_training_progress(self.log_noisy, title=\"Stochastic policy\")\n",
        "            self.__visualize_training_progress(self.log_greedy,title=\"Greedy policy\")\n",
        "        \n",
        "        if not save_name is None:\n",
        "            # Save the agent in a (pickle) file.\n",
        "            # We could also save the logs, but frankly I would rather \n",
        "            # use an external logger/tracking tool like Tensorboard or Neptune.\n",
        "            self.save_agent(save_name)\n",
        "\n",
        "        return self.log_noisy, self.log_greedy\n",
        "\n",
        "    def test(self, agent=None, Nepisodes=1000, summary=True):\n",
        "        \"\"\"Train an agent.\n",
        "\n",
        "        agent can be:\n",
        "        None -- use the agent that is stored in self.agent\n",
        "        str  -- load the agent from a file \n",
        "        an instance of a class that provides methods new_episode, select_action, and update, see discreteQlearningAgent class.\n",
        "\n",
        "        In contrast to train method, this method will not change self.agent, so it can be used to test \n",
        "        different agents while keeping a single agent for training.\n",
        "        \"\"\"\n",
        "        rhist = []\n",
        "        lhist = []\n",
        "        if agent is None:\n",
        "            agent = self.agent\n",
        "        else:\n",
        "            agent = self.load_agent(agent, remember_agent=False)\n",
        "\n",
        "        for i in range(Nepisodes):\n",
        "            env.reset()\n",
        "            \n",
        "            try:\n",
        "                agent.new_episode()\n",
        "            except AttributeError:\n",
        "                raise Exception(\"agent: method new_episode not implemented correctly.\")\n",
        "\n",
        "            for j in range(MAX_LEN_EPISODE):\n",
        "                # Select an action according to the greedy policy.\n",
        "                try:\n",
        "                    action = agent.select_action(greedy=True)\n",
        "                except AttributeError:\n",
        "                    raise Exception(\"agent: method select_action not implemented correctly.\")\n",
        "                # Take a step in the environment using our selected action and collect observations.\n",
        "                obs, r, done, info = env.step(action)\n",
        "                # Update agent's state (but do not train!)\n",
        "                try:\n",
        "                    agent.update(obs, r, done, learn=False)\n",
        "                except AttributeError:\n",
        "                    raise Exception(\"agent: method update not implemented correctly.\")\n",
        "\n",
        "                if done:\n",
        "                    rhist.append(r)\n",
        "                    lhist.append(j+1)\n",
        "                    break\n",
        "                if j+1==MAX_LEN_EPISODE:\n",
        "                    raise Exception(f\"The length of the episode has hit the maximum number: {MAX_LEN_EPISODE}\")\n",
        "        \n",
        "        if summary:\n",
        "            rhist_np = np.asarray( rhist )\n",
        "            r_mean = np.mean( rhist_np )\n",
        "            #r_std  = np.std( rhist_np ) # We can infer this from r_mean for binary (Bernoulli) random variable\n",
        "            \n",
        "            lhist_np = np.asarray( lhist )\n",
        "            l_mean = np.mean( lhist_np )\n",
        "            l_std  = np.std( lhist_np )\n",
        "        \n",
        "            print(f\"Win rate: {100*r_mean:.1f} \\u00B1 {100*3*np.sqrt(r_mean*(1-r_mean))/np.sqrt(len(rhist_np)):.1f} %\")\n",
        "            print(f\"Average number of steps: {l_mean:.1f} \\u00B1 {3*l_std/np.sqrt(len(lhist_np)):.1f}\")\n",
        "        return rhist, lhist\n",
        "\n",
        "    def save_agent(self, fname):\n",
        "        \"\"\"Save an object (agent) to a file (fname).\n",
        "        \n",
        "        Note that this function can save other objects too, \n",
        "        since we are not checking here if the objects is an \n",
        "        instance of a class that is consistent with \n",
        "        the agent class. \n",
        "        \"\"\"\n",
        "        pickle.dump(self.agent, file=open(fname, \"wb\"))\n",
        "        \n",
        "    def load_agent(self, agent_name, remember_agent=True):\n",
        "        \"\"\"Load and return an object (agent). \n",
        "\n",
        "        If agent_name is a string, load the agent from a pickle file.\n",
        "        If loading the file fails, set agent as None.\n",
        " \n",
        "        If agent_name is not a string this function assumes \n",
        "        that it is an instance of the class discreteQlearningAgent \n",
        "        and loads it as such (setting self.agent to agent_name).\n",
        "\n",
        "        The flag remember_agent is used to determine if self.agent should be replaced. \n",
        "        This is useful to switch between two alternative behaviors: \n",
        "        loading an agent temporarily (for tests), or loading a new agent for training. \n",
        "\n",
        "        Note that this function can load other objects too, \n",
        "        since we are not checking here if the objects is an \n",
        "        instance of a class that is consistent with \n",
        "        the agent class. \n",
        "        \n",
        "        TO DO: check if the loaded object is an instance of the class...\n",
        "        \"\"\"\n",
        "\n",
        "        if isinstance(agent_name, str):\n",
        "            try:\n",
        "                agent = pickle.load(file=open(agent_name, \"rb\"))\n",
        "            except FileNotFoundError:\n",
        "                print(f\"File {agent_name} does not exist.\") \n",
        "                agent = None\n",
        "        else:\n",
        "            agent = agent_name\n",
        "        \n",
        "        if remember_agent:\n",
        "            self.agent = agent\n",
        "\n",
        "        return agent\n",
        "    \n",
        "    def reset_logs(self):\n",
        "        \"\"\"Empty the lists containing logs.\"\"\"\n",
        "        self.log_greedy = []\n",
        "        self.log_noisy  = []\n",
        "\n",
        "    def __visualize_training_progress(self, log_list, title=\"\", tau1=100, tau2=1000):\n",
        "        \"\"\" Visualize training progress. \"\"\"\n",
        "        \n",
        "        # List of tuples --> ndarray.\n",
        "        # log[:,0] -- 'time steps' (episode #)\n",
        "        # log[:,1] -- rewards\n",
        "        # log[:,2] -- length of the episode\n",
        "        log = np.array( log_list )\n",
        "\n",
        "        fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
        "        # Plot (low-pass filtered) rewards\n",
        "        ax[0].plot(log[:,0], self.__low_pass(log[:,1], tau=tau1))\n",
        "        ax[0].plot(log[:,0], self.__low_pass(log[:,1], tau=tau2))\n",
        "        ax[0].set_xlabel('# of episodes')\n",
        "        ax[0].set_ylabel('average (low-pass filtered) reward')\n",
        "        # Plot (low-pass filtered) number of steps \n",
        "        ax[1].plot(log[:,0], self.__low_pass(log[:,2], tau=tau1))\n",
        "        ax[1].plot(log[:,0], self.__low_pass(log[:,2], tau=tau2))\n",
        "        ax[1].set_xlabel('# of episodes')\n",
        "        ax[1].set_ylabel('average # of steps per episode')\n",
        "\n",
        "        fig.suptitle(title, fontsize=14)\n",
        "\n",
        "    def __low_pass(self, x, tau=1):\n",
        "        \"\"\"Calculate and return a first-order linear low-pass filter of a 1D time series. \n",
        "\n",
        "        Parameters:\n",
        "        x -- 1D numpy array with the time series to be filtered\n",
        "        tau -- time constant of the filter\n",
        "        \n",
        "        Returns: 1D numpy array with the filtered signal (same length as x)\n",
        "        \"\"\"\n",
        "        y = np.empty( (len(x)) )\n",
        "        y[0] = np.mean(x[:tau])     # Initialize the first element with an average.\n",
        "        a = 1/tau\n",
        "\n",
        "        for i in range(len(x)-1):\n",
        "            y[i+1] = (1-a)*y[i] + a*x[i]\n",
        "\n",
        "        return y\n",
        "\n",
        "# Create an instance of the FrozeLake8x8-v0 environment\n",
        "env = gym.make('FrozenLake8x8-v0')#, map_name=None)\n",
        "# Instantiate an agent (discreteQlearningAgent)\n",
        "agent = discreteQlearningAgent(alpha_tau=3000)\n",
        "# Instantiate a gymTrainer\n",
        "trainer = gymTrainer(env, agent)\n",
        "# It is also possible to specify the path with a pickle containing the agent:\n",
        "# trainer = gymTrainer(env, \"agentQ.pickle\")\n",
        "\n",
        "# Train the agent for Nepisodes steps. By default this will also visualize the training.\n",
        "# If save_name is specified as a string (filename), \n",
        "# the agent will be automatically saved to the file (pickle) at the end of the training. \n",
        "trainer.train(Nepisodes=20000, save_name=\"agent0.pickle\")\n",
        "\n",
        "# Test the trained agent in the trainer.\n",
        "print(\"== Trained agent (tabular Q-learning): ==\")\n",
        "trainer.test()\n",
        "\n",
        "# Test another agent (loaded externally) using the same trainer. Note that this will not update the agent for future training.\n",
        "print(\"== Simple handcrafted agent: ==\")\n",
        "agentHC = testAgent()   # Handcrafted agent\n",
        "trainer.test(agentHC)\n",
        "\n",
        "# Test another agent loaded directly from a specified path.\n",
        "trainer.test(\"agent0.pickle\")\n",
        "\n",
        "# Save the trained agent whenever needed. \n",
        "trainer.save_agent(\"agent1.pickle\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yob1W4f6neFt"
      },
      "source": [
        "trainer_new = gymTrainer(env, \"agentQ.pickle\")\n",
        "trainer_new.test();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUsVxe1SkD3A"
      },
      "source": [
        "trainer.test(6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGgty11Id5bK"
      },
      "source": [
        "print(np.__version__)\n",
        "import matplotlib as mpl\n",
        "print(mpl.__version__)\n",
        "print(gym.__version__)\n",
        "print(pickle.format_version)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clJgaVggM14J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}